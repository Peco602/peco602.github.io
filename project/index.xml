<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Projects | Giovanni Pecoraro</title><link>https://www.peco602.com/project/</link><atom:link href="https://www.peco602.com/project/index.xml" rel="self" type="application/rss+xml"/><description>Projects</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Wed, 15 Mar 2023 00:00:00 +0000</lastBuildDate><image><url>https://www.peco602.com/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url><title>Projects</title><link>https://www.peco602.com/project/</link></image><item><title>Archive-To-Images</title><link>https://www.peco602.com/project/0050-archive-to-images/</link><pubDate>Wed, 15 Mar 2023 00:00:00 +0000</pubDate><guid>https://www.peco602.com/project/0050-archive-to-images/</guid><description>&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>Since some cloud providers offer free unlimited picture-only storage, the &lt;strong>Archive-To-Images&lt;/strong> library allows to convert any collection of files into pictures to be uploaded without any additional cost.&lt;/p>
&lt;h2 id="installation">Installation&lt;/h2>
&lt;p>The package can be easily installed via &lt;code>pip&lt;/code> package manager:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">$ pip install archive-to-images
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="usage-as-cli">Usage as CLI&lt;/h2>
&lt;h3 id="transform-to-images">Transform to images&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">$ archive-to-images transform --help
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> Usage: archive-to-images transform &lt;span class="o">[&lt;/span>OPTIONS&lt;span class="o">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> Transforms an archive into multiple images.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">╭─ Options ────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">│ * --path -p TEXT Path containing data to be archived. &lt;span class="o">[&lt;/span>default: None&lt;span class="o">]&lt;/span> &lt;span class="o">[&lt;/span>required&lt;span class="o">]&lt;/span> │
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">│ * --name -n TEXT Name of the archive. &lt;span class="o">[&lt;/span>default: None&lt;span class="o">]&lt;/span> &lt;span class="o">[&lt;/span>required&lt;span class="o">]&lt;/span> │
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">│ --size -s &lt;span class="o">[&lt;/span>0.5&lt;span class="p">|&lt;/span>1&lt;span class="p">|&lt;/span>2&lt;span class="p">|&lt;/span>5&lt;span class="p">|&lt;/span>10&lt;span class="o">]&lt;/span> Maximum size of an image in MB. &lt;span class="o">[&lt;/span>default: 1&lt;span class="o">]&lt;/span> │
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">│ --encrypt -e Protect archive with password. │
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">│ --verbose -v Enable verbose output. │
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">│ --help Show this message and exit. │
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Create an image collection from data contained in multiple paths.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">$ archive-to-images transform --path /home/alice/Desktop --path /home/alice/Documents --name ARCHIVE_ALICE
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Set the maximum image size in MB (default: 1):&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">$ archive-to-images transform --path /home/alice/Desktop --path /home/alice/Documents --name ARCHIVE_ALICE -s &lt;span class="m">5&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Encrypt data with a password:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">$ archive-to-images transform --path /home/alice/Desktop --path /home/alice/Documents --name ARCHIVE_ALICE -s &lt;span class="m">5&lt;/span> -e
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="restore-from-images">Restore from images&lt;/h3>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">$ archive-to-images restore --help
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> Usage: archive-to-images restore &lt;span class="o">[&lt;/span>OPTIONS&lt;span class="o">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> Restores an archive from multiple images.
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">╭─ Options ────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">│ * --path -p TEXT Path containing images to be processed. &lt;span class="o">[&lt;/span>default: None&lt;span class="o">]&lt;/span> &lt;span class="o">[&lt;/span>required&lt;span class="o">]&lt;/span> │
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">│ --verbose -v Enable verbose output. │
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">│ --help Show this message and exit. │
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">╰──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Restore the archives stored in image collections:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">$ archive-to-images restore --path /home/alice/Downloads/Album1 --path /home/alice/Downloads/Album2
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The library will automatically find all the archives stored in the images and will output a &lt;code>zip&lt;/code> archive for each one.&lt;/p>
&lt;h2 id="usage-as-docker">Usage as docker&lt;/h2>
&lt;p>Run the docker image and bind the current folder to the &lt;code>workspace&lt;/code> path inside the container:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">$ docker run -it --rm -v &lt;span class="k">$(&lt;/span>&lt;span class="nb">pwd&lt;/span>&lt;span class="k">)&lt;/span>:/workspace peco602/archive_to_images:latest bash
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>then it is possible to use the CLI directly from the container bash.&lt;/p></description></item><item><title>Maternal Health Risk Predictor</title><link>https://www.peco602.com/project/0040-maternal-health-mlops/</link><pubDate>Tue, 06 Sep 2022 00:00:00 +0000</pubDate><guid>https://www.peco602.com/project/0040-maternal-health-mlops/</guid><description>&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>According to the World Health Organization (WHO):&lt;/p>
&lt;p>&amp;ldquo;&lt;em>Maternal health refers to the health of women during pregnancy, childbirth and the post-natal period. Each stage should be a positive experience, ensuring women and their babies reach their full potential for health and well-being. Although important progress has been made in the last two decades, about 295 000 women died during and following pregnancy and childbirth in 2017. This number is unacceptably high. The most common direct causes of maternal injury and death are excessive blood loss, infection, high blood pressure, unsafe abortion, and obstructed labour, as well as indirect causes such as anemia, malaria, and heart disease. Most maternal deaths are preventable with timely management by a skilled health professional working in a supportive environment. Ending preventable maternal death must remain at the top of the global agenda. At the same time, simply surviving pregnancy and childbirth can never be the marker of successful maternal health care. It is critical to expand efforts reducing maternal injury and disability to promote health and well-being. Every pregnancy and birth is unique. Addressing inequalities that affect health outcomes, especially sexual and reproductive health and rights and gender, is fundamental to ensuring all women have access to respectful and high-quality maternity care.&lt;/em>&amp;rdquo;&lt;/p>
&lt;p>The goal of the project is to apply what has been learned during the MLOps Zoomcamp course to build a MLOps pipeline for woman health risk prediction during pregnancy.&lt;/p>
&lt;h2 id="dataset">Dataset&lt;/h2>
&lt;p>The dataset used to feed the MLOps pipeline has been downloaded from &lt;a href="https://www.kaggle.com/datasets/pyuxbhatt/maternal-health-risk" target="_blank" rel="noopener">Kaggle&lt;/a> and contains data collected from several hospitals, community clinics and maternal health cares through an IoT-based risk monitoring system. The dataset is updated daily and is characterized by the following features:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Feature&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Age&lt;/td>
&lt;td>Age when a woman is pregnant.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>SystolicBP&lt;/td>
&lt;td>Upper value of blood pressure.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>DiastolicBP&lt;/td>
&lt;td>Lower value of blood pressure.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>BS&lt;/td>
&lt;td>Blood glucose levels in terms of molar concentration.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>HeartRate&lt;/td>
&lt;td>A normal resting heart rate.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>BodyTemp&lt;/td>
&lt;td>Average human body temperature.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Risk Level&lt;/td>
&lt;td>Predicted risk intensity level during pregnancy considering the previous attributes.&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="mlops-pipeline">MLOps pipeline&lt;/h2>
&lt;h3 id="architecture">Architecture&lt;/h3>
&lt;img src="./architecture.png" width="100%"/>
&lt;h3 id="deployment">Deployment&lt;/h3>
&lt;p>The MLOps pipeline is fully dockerised and can be easily deployed via the following steps:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Clone the &lt;code>maternal-health-risk&lt;/code> repository locally:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">$ git clone https://github.com/Peco602/maternal-health-risk.git
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>Install the pre-requisites necessary to run the pipeline:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">$ &lt;span class="nb">cd&lt;/span> maternal-health-risk
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">$ sudo apt install make
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">$ make prerequisites
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>It is also suggested to add the current user to the &lt;code>docker&lt;/code> group to avoid running the next steps as &lt;code>sudo&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">$ sudo groupadd docker
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">$ sudo usermod -aG docker &lt;span class="nv">$USER&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>then, logout and log back in so that the group membership is re-evaluated.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>[&lt;em>Optional&lt;/em>] Configure the development evironment:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">$ make setup
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This is required to perform further development and testing on the pipeline.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>[&lt;em>Optional&lt;/em>] Insert Kaggle credentials in the &lt;code>.env&lt;/code> file to allow the automatic scheduled dataset update:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Kaggle credentials&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nv">KAGGLE_USERNAME&lt;/span>&lt;span class="o">=&lt;/span>*****
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nv">KAGGLE_KEY&lt;/span>&lt;span class="o">=&lt;/span>*****
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>In case the credentials are not available, the training dataset &lt;code>data/data.csv&lt;/code> must be updated manually.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Pull the Docker images:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">$ make pull
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>Launch the MLOps pipeline:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">$ make run
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Once ready, the following services will be available:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Service&lt;/th>
&lt;th>Port&lt;/th>
&lt;th>Interface&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Web Application&lt;/td>
&lt;td>80&lt;/td>
&lt;td>0.0.0.0&lt;/td>
&lt;td>Prediction web service (see picture below)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Prefect&lt;/td>
&lt;td>4200&lt;/td>
&lt;td>127.0.0.1&lt;/td>
&lt;td>Training workflow orchestration&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MLFlow&lt;/td>
&lt;td>5000&lt;/td>
&lt;td>127.0.0.1&lt;/td>
&lt;td>Experiment tracking and model registry&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MinIO&lt;/td>
&lt;td>9001&lt;/td>
&lt;td>127.0.0.1&lt;/td>
&lt;td>S3-equivalent bucket management&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Evidently&lt;/td>
&lt;td>8085&lt;/td>
&lt;td>127.0.0.1&lt;/td>
&lt;td>Data and target drift report generation (&lt;code>/dashboard&lt;/code> route)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Grafana&lt;/td>
&lt;td>3000&lt;/td>
&lt;td>127.0.0.1&lt;/td>
&lt;td>Data and target drift real-time dashboards&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;img src="./webservice.png" width="100%"/>
&lt;/li>
&lt;/ol>
&lt;h3 id="training">Training&lt;/h3>
&lt;p>Once the MLOps pipeline has been started, the prediction web service can already work thanks to a default pre-trained model available in the Docker image. In order to enable pipeline training workflow it is necessary to create a scheduled Prefect deployment via:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">$ make deployment
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The training workflow will be then automatically executed every day. It will download the latest dataset (if the Kaggle credentials have been provided), search the best model in terms of accuracy among XGBoost, Support Vector Machine and Random Forest and finally will store it in the model registry. It is worth noting the training workflow can also be immediately executed without waiting the next schedule:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">$ make train
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Once the updated model is ready, it can be moved to production by restarting the pipeline:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">$ make restart
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>the web service will automatically connect to the registry and get the most updated model. If the model is still not available, it will continue to use the default one.&lt;/p>
&lt;h3 id="monitoring">Monitoring&lt;/h3>
&lt;p>It is possible to generate simulated traffic via:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">$ make generate-traffic
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Then, the prediction service can be monitored via:&lt;/p>
&lt;ul>
&lt;li>Grafana (in real-time): &lt;code>http://127.0.0.1:3000&lt;/code>&lt;/li>
&lt;li>Evidently (for report generation): &lt;code>http://127.0.0.1:8085/dashboard&lt;/code>&lt;/li>
&lt;/ul>
&lt;h3 id="disposal">Disposal&lt;/h3>
&lt;p>The MLOps pipeline can be disposed via:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">$ make kill
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>while the Docker volumes used for persistence can be removed via:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">$ make clean
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="github-actions">GitHub Actions&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>Continuous Integration&lt;/strong>: On every push and pull request on &lt;code>main&lt;/code> and &lt;code>dev&lt;/code> branches, the Docker images are built, tested and then pushed to DockerHub.&lt;/li>
&lt;li>&lt;strong>Continuous Deployment&lt;/strong>: On every push and pull request on &lt;code>main&lt;/code> branch, only if the Continuous Integration workflow has been successful successful, the updated pipeline is deployed to the target server and run.&lt;/li>
&lt;/ul>
&lt;h2 id="applied-technologies">Applied technologies&lt;/h2>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Name&lt;/th>
&lt;th>Scope&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Jupyter Notebooks&lt;/td>
&lt;td>Exploratory data analysis and pipeline prototyping.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Docker&lt;/td>
&lt;td>Application containerization.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Docker-Compose&lt;/td>
&lt;td>Multi-container Docker applications definition and running.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Prefect&lt;/td>
&lt;td>Workflow orchestration.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MLFlow&lt;/td>
&lt;td>Experiment tracking and model registry.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>PostgreSQL&lt;/td>
&lt;td>MLFLow experiment tracking database.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MinIO&lt;/td>
&lt;td>High Performance Object Storage compatible with Amazon S3 cloud storage service.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Flask&lt;/td>
&lt;td>Web server.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Bootstrap&lt;/td>
&lt;td>Frontend toolkit.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MongoDB&lt;/td>
&lt;td>Prediction database.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>EvidentlyAI&lt;/td>
&lt;td>ML models evaluation and monitoring.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Prometheus&lt;/td>
&lt;td>Time Series Database for ML models real-time monitoring.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Grafana&lt;/td>
&lt;td>ML models real-time monitoring dashboards.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>pytest&lt;/td>
&lt;td>Python unit testing suite.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>pylint&lt;/td>
&lt;td>Python static code analysis.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>black&lt;/td>
&lt;td>Python code formatting.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>isort&lt;/td>
&lt;td>Python import sorting.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Pre-Commit Hooks&lt;/td>
&lt;td>Simple code issue identification before submission.&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>GitHub Actions&lt;/td>
&lt;td>CI/CD pipelines.&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="disclaimer">Disclaimer&lt;/h2>
&lt;p>This prediction service has been developed as the final project of the MLOps Zoomcamp course from DataTalks.Club. It does not provide medical advice and it is intended for informational purposes only. It cannot be considered a substitute for professional medical advice, diagnosis or treatment. Never ignore professional medical advice in seeking treatment because of something you have read here.&lt;/p></description></item><item><title>FindWall</title><link>https://www.peco602.com/project/0030-findwall/</link><pubDate>Fri, 20 May 2022 00:00:00 +0000</pubDate><guid>https://www.peco602.com/project/0030-findwall/</guid><description>&lt;h2 id="what-does-it-do">What does it do?&lt;/h2>
&lt;p>FindWall is Python script that allows to understand if your network provider is limiting your access to the Internet by blocking any TCP/UDP port. In order to perform this check FindWall needs to connect a public VPS of your property. FindWall performs the following actions:&lt;/p>
&lt;ol>
&lt;li>Connects to the VPS via SSH&lt;/li>
&lt;li>Opens a port in listening mode&lt;/li>
&lt;li>Tries to connect to that port from the local machine&lt;/li>
&lt;li>Closes the port&lt;/li>
&lt;/ol>
&lt;h2 id="how-do-you-use-it">How do you use it?&lt;/h2>
&lt;img src="./demo.gif" width="100%"/>
&lt;p>To use FindWall you just need an account on a public VPS. The account must have root access if you want to test ports in the range &lt;code>1-1024&lt;/code>. The root account is also required to automatically install the tool &lt;code>nc&lt;/code> to open ports.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">$ pip install -r requirements
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">$ python findwall.py --help
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">=====================================================================================
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> ███████╗██╗███╗ ██╗██████╗ ██╗ ██╗ █████╗ ██╗ ██╗
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> ██╔════╝██║████╗ ██║██╔══██╗██║ ██║██╔══██╗██║ ██║
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> █████╗ ██║██╔██╗ ██║██║ ██║██║ █╗ ██║███████║██║ ██║
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> ██╔══╝ ██║██║╚██╗██║██║ ██║██║███╗██║██╔══██║██║ ██║
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> ██║ ██║██║ ╚████║██████╔╝╚███╔███╔╝██║ ██║███████╗███████╗
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> ╚═╝ ╚═╝╚═╝ ╚═══╝╚═════╝ ╚══╝╚══╝ ╚═╝ ╚═╝╚══════╝╚══════╝
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">=====================================================================================
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">usage: findwall.py [-h] --ssh-host SSH_HOST [--ssh-port SSH_PORT] --ssh-username SSH_USERNAME [--ssh-password SSH_PASSWORD] [--ask-ssh-pass] [--ssh-key SSH_KEY] --ports PORTS [--udp]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> [--threads THREADS]
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Check if someone is blocking you!
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">optional arguments:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> -h, --help show this help message and exit
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> --ssh-host SSH_HOST Remote host
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> --ssh-port SSH_PORT Remote SSH port
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> --ssh-username SSH_USERNAME
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> Remote SSH username
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> --ssh-password SSH_PASSWORD
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> Remote SSH password
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> --ask-ssh-pass Ask for remote SSH password
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> --ssh-key SSH_KEY Remote SSH private key
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> --ports PORTS Port range to scan (default: 1-1024)
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> --udp Scan in UDP
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> --threads THREADS Number of threads (default: 1)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>As an example:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">&lt;span class="line">&lt;span class="cl">$ python findwall.py --ssh-host 172.17.0.2 --ssh-port 22 --ssh-username test --ssh-password test --ports 8000-8010 --threads 3
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>RustHunter</title><link>https://www.peco602.com/project/0020-rusthunter/</link><pubDate>Sun, 15 May 2022 00:00:00 +0000</pubDate><guid>https://www.peco602.com/project/0020-rusthunter/</guid><description>&lt;p>RustHunter is a modular incident response framework to build and compare environmental baselines. It is written in Rust and uses Ansible to collect data across multiple hosts.&lt;/p>
&lt;p>Due to the following features it can be also employed to perform threat hunting and incident handling:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Multi-Platform&lt;/strong>: it is able to collect data from both Windows, Linux and macOS machines;&lt;/li>
&lt;li>&lt;strong>Agentless&lt;/strong>: the usage of the Ansible technology based on SSH and WinRM allows to overcome the requirement of a local agent waiting for a task to be accomplished;&lt;/li>
&lt;li>&lt;strong>Easily Deployable&lt;/strong>: it is cross-platform and can be deployed both on premises and in a Cloud-based environment. A Bash and a PowerShell scripts have been developed to execute the tool respectively from a Linux and Windows machine;&lt;/li>
&lt;li>&lt;strong>Easily Expandable&lt;/strong>: it provides developer-ready Rust specifications offering an easy way to expand the product features by writing custom modules to collect additional machine data.&lt;/li>
&lt;/ul></description></item><item><title>Cobalt Strike Aggressor Scripts</title><link>https://www.peco602.com/project/0010-cobaltstrike-aggressor-scripts/</link><pubDate>Mon, 03 Jan 2022 00:00:00 +0000</pubDate><guid>https://www.peco602.com/project/0010-cobaltstrike-aggressor-scripts/</guid><description>&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>This is a collection of Cobalt Strike Aggressor scripts I developed and tested while I was a Red Team member for Locked Shields 2021.&lt;/p>
&lt;h2 id="initial-access">Initial Access&lt;/h2>
&lt;p>&lt;a href="https://attack.mitre.org/tactics/TA0001/" target="_blank" rel="noopener">Initial Access&lt;/a> consists of techniques that use various entry vectors to gain their initial foothold within a network.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;code>initial-access-cmd/initial-access-cmd.cna&lt;/code>:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Certutil Web Delivery (Custom)&lt;/strong>: Provides a CMD one-liner to deliver a custom executable via Certutil&lt;/li>
&lt;li>&lt;strong>Certutil Web Delivery (Stageless)&lt;/strong>: Provides a CMD one-liner to deliver a stageless Cobalt Strike payload via Certutil&lt;/li>
&lt;li>&lt;strong>Bitsadmin Web Delivery (Stageless)&lt;/strong>: Provides a CMD one-liner to deliver a stageless Cobalt Strike payload via Bitsadmin&lt;/li>
&lt;li>&lt;strong>Regsvr32 Web Delivery (Stageless)&lt;/strong>: Provides a CMD one-liner to deliver a stageless Cobalt Strike payload via Regsvr32&lt;/li>
&lt;li>&lt;strong>MSHTA Web Delivery (Stageless)&lt;/strong>: Provides a CMD one-liner to deliver a stageless Cobalt Strike payload via MSHTA&lt;/li>
&lt;li>&lt;strong>Rundll32 Web Delivery (Stageless)&lt;/strong>: Provides a CMD one-liner to deliver a stageless Cobalt Strike payload via Rundll32&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;code>initial-access-powershell/initial-access-powershell.cna&lt;/code>:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Pure Powershell Web Delivery (Stageless)&lt;/strong>: Provides a PowerShell one-liner to deliver (in-memory) a stageless Cobalt Strike PoweShell payload&lt;/li>
&lt;li>&lt;strong>Artifact Powershell Web Delivery (Stageless)&lt;/strong>: Provides a PowerShell one-liner to deliver (in-memory) a PowerShell scripts which embeds a stageless Cobalt Strike payload&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;code>initial-access-python/initial-access-python.cna&lt;/code>:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Python 2 Web Delivery&lt;/strong>: Provides a Python 2 one-liner to deliver a stageless Cobalt Strike payload (it assumes the following path for Python 2: &lt;em>c:\Python27\pythonw.exe&lt;/em>)&lt;/li>
&lt;li>&lt;strong>Python 3 Web Delivery&lt;/strong>: Provides a Python 3.9 one-liner to deliver a stageless Cobalt Strike payload (it assumes the following path for Python 3.9: &lt;em>C:\Python39\pythonw.exe&lt;/em>)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="persistence">Persistence&lt;/h2>
&lt;p>&lt;a href="https://attack.mitre.org/tactics/TA0003/" target="_blank" rel="noopener">Persistence&lt;/a> consists of techniques that adversaries use to keep access to systems across restarts, changed credentials, and other interruptions that could cut off their access.&lt;/p>
&lt;ul>
&lt;li>&lt;code>persistence-sharpersist/persistence-sharpersist.cna&lt;/code>:
&lt;ul>
&lt;li>&lt;strong>* Startup Folder (Upload executable) [Reboot]&lt;/strong>: Installs persistence for all users by uploading an executable to the startup folder [Requires administrator privileges]&lt;/li>
&lt;li>&lt;strong>Startup Folder (Upload executable) [Reboot]&lt;/strong>: Installs persistence for the current user by uploading an executable to the startup folder&lt;/li>
&lt;li>&lt;strong>* Windows Service (Powershell command) [Reboot]&lt;/strong>: Installs persistence for all users by creating a Windows service launching a PowerShell command [Requires administrator privileges]&lt;/li>
&lt;li>&lt;strong>* Windows Service (Upload executable) [Reboot]&lt;/strong>: Installs persistence for all users by uploading an executable and creating a Windows service launching it [Requires administrator privileges]&lt;/li>
&lt;li>&lt;strong>* Scheduled Task (Powershell command) [Logon/Hourly]&lt;/strong>: Installs persistence for all users by creating a Scheduled Task launching a PowerShell command [Requires administrator privileges]&lt;/li>
&lt;li>&lt;strong>* Scheduled Task (Upload executable) [Logon/Hourly]&lt;/strong>: Installs persistence for all users by uploading an executable and creating a Scheduled Task launching it [Requires administrator privileges]&lt;/li>
&lt;li>&lt;strong>Scheduled Task (Powershell command) [Logon/Hourly]&lt;/strong>: Installs persistence for the current user by creating a Scheduled Task launching a PowerShell command&lt;/li>
&lt;li>&lt;strong>Scheduled Task (Upload executable) [Logon/Hourly]&lt;/strong>: Installs persistence for the current user by uploading an executable and creating a Scheduled Task launching it&lt;/li>
&lt;li>&lt;strong>* Registry (Powershell command) [Logon]&lt;/strong>: Installs persistence for all users by adding a PowerShell command to an autorun registry key [Requires administrator privileges]&lt;/li>
&lt;li>&lt;strong>* Registry (Upload executable) [Logon]&lt;/strong>: Installs persistence for all users by uploading an executable and adding it to an autorun registry key [Requires administrator privileges]&lt;/li>
&lt;li>&lt;strong>Registry (Powershell command) [Logon]&lt;/strong>: Installs persistence for the current user by adding a PowerShell command to an autorun registry key [Requires administrator privileges]&lt;/li>
&lt;li>&lt;strong>Registry (Upload executable) [Logon]&lt;/strong>: Installs persistence for the current user by uploading an executable and adding it to an autorun registry key&lt;/li>
&lt;li>&lt;strong>* Sticky Keys (CMD)&lt;/strong>: Launches a CMD prompt in case of sticky keys or other accessibility tools (e.g., Narrator, Magnifier) execution&lt;/li>
&lt;li>&lt;strong>* Sticky Keys (Beacon)&lt;/strong>: Launches a Cobalt Strike beacon in case of sticky keys or other accessibility tools (e.g., Narrator, Magnifier) execution&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="defense-evasion">Defense Evasion&lt;/h2>
&lt;p>&lt;a href="https://attack.mitre.org/tactics/TA0005/" target="_blank" rel="noopener">Defense Evasion&lt;/a> consists of techniques that adversaries use to avoid detection throughout their compromise. Techniques used for defense evasion include uninstalling/disabling security software or obfuscating/encrypting data and scripts.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;code>evasion-disable-defender/evasion-disable-defender.cna&lt;/code>:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>* Disable AV/Firewall&lt;/strong>: Disables Windows Defender [Requires administrator privileges]&lt;/li>
&lt;li>&lt;strong>* Add Exclusions (Auto)&lt;/strong>: Automatically adds a list of paths and executables to the Windows Defender exclusions [Requires administrator privileges]&lt;/li>
&lt;li>&lt;strong>* Add Exclusions (Custom)&lt;/strong>: Adds a custom path and executable to the Windows Defender exclusions [Requires administrator privileges]&lt;/li>
&lt;li>&lt;strong>* Add Exclusions (Extensions)&lt;/strong>: Adds a custom file extension to the Windows Defender exclusions [Requires administrator privileges]&lt;/li>
&lt;li>&lt;strong>* Remove Definitions&lt;/strong>: Removes Windows Defender definitions [Requires administrator privileges]&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;code>evasion-disable-edr/evasion-disable-edr.cna&lt;/code>&lt;/p>
&lt;ul>
&lt;li>&lt;strong>* Kill EDRs&lt;/strong>: Tries to automatically kill all EDRs/AVs [Requires administrator privileges]&lt;/li>
&lt;li>&lt;strong>* Kill EDR (Custom)&lt;/strong>: Tries to kill a custom EDR/AV [Requires administrator privileges]&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul></description></item><item><title>RedHerd Framework</title><link>https://www.peco602.com/project/0000-redherd-framework/</link><pubDate>Thu, 15 Jul 2021 00:00:00 +0000</pubDate><guid>https://www.peco602.com/project/0000-redherd-framework/</guid><description>&lt;p>RedHerd is a collaborative and serverless framework for orchestrating a geographically distributed group of assets capable of conducting simulating complex offensive cyberspace operations.&lt;/p>
&lt;iframe width="100%" height="400" src="https://www.youtube.com/embed/-AnJBcTwR8Q" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" style="margin-bottom: 30px;" allowfullscreen>&lt;/iframe>
&lt;p>The framework takes advantage of the &amp;ldquo;as a Service&amp;rdquo; paradigm in order to deploy a ready-to-use infrastructure that can also be adopted for effective training purposes, by reliably reproducing a real-world cyberspace scenario in which red and blue teams can challenge each other. RedHerd perfectly fits the Open Systems Architecture design pattern, thanks to the adoption of both open standards and wide-spread open source software components.&lt;/p></description></item></channel></rss>